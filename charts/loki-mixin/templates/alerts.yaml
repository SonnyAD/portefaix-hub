---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ printf "%s-%s" (include "loki-mixin.fullname" .) "alerts" | trunc 63 | trimSuffix "-" }}
  annotations:
{{ include "loki-mixin.annotations" . | indent 4 }}
{{- if .Values.additionalAnnotations }}
{{ toYaml .Values.additionalAnnotations | indent 4 }}
{{- end }}
  labels:
    app: {{ include "loki-mixin.name" . }}
{{ include "loki-mixin.labels" . | indent 4 }}
{{- if .Values.additionalLabels }}
{{ toYaml .Values.additionalLabels | indent 4 }}
{{- end }}
spec:
  groups:
  - name: loki_alerts
    rules:
    - alert: LokiRequestErrors
      annotations:
        message: |
          {{`{{`}} $labels.job {{`}}`}} {{`{{`}} $labels.route {{`}}`}} is experiencing {{`{{`}} printf "%.2f" $value {{`}}`}}% errors.
      expr: |
        100 * sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[1m])) by (namespace, job, route)
          /
        sum(rate(loki_request_duration_seconds_count[1m])) by (namespace, job, route)
          > 10
      for: 15m
      labels:
        severity: critical
    - alert: LokiRequestPanics
      annotations:
        message: |
          {{`{{`}} $labels.job {{`}}`}} is experiencing {{`{{`}} printf "%.2f" $value {{`}}`}}% increase of panics.
      expr: |
        sum(increase(loki_panic_total[10m])) by (namespace, job) > 0
      labels:
        severity: critical
    - alert: LokiRequestLatency
      annotations:
        message: |
          {{`{{`}} $labels.job {{`}}`}} {{`{{`}} $labels.route {{`}}`}} is experiencing {{`{{`}} printf "%.2f" $value {{`}}`}}s 99th percentile latency.
      expr: |
        namespace_job_route:loki_request_duration_seconds:99quantile{route!~"(?i).*tail.*"} > 1
      for: 15m
      labels:
        severity: critical
